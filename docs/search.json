[
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "1  Creating a ggplot",
    "section": "",
    "text": "#Basics of ggplot2\nThis chapter focuses on ggplot2, one of the core packages in the tidyverse. Load the tidyverse by running:\n\npacman::p_load(\"tidyverse\",\n               \"ggthemes\"\n               )\n\nWe will use penguins data frame. A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). penguins contains 344 observations.\n\npenguins &lt;- read.csv(\"data/penguins.csv\")\n\n\nFor an alternative view, where you can see all variables and the first few observations of each variable, use glimpse(). Or by using view(), you can see the data in a different tab with all information.\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 9\n$ rowid             &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ species           &lt;chr&gt; \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"Adelie\", \"A…\n$ island            &lt;chr&gt; \"Torgersen\", \"Torgersen\", \"Torgersen\", \"Torgersen\", …\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;chr&gt; \"male\", \"female\", \"female\", NA, \"female\", \"male\", \"f…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nview(penguins)\n\nLet’s recreate this plot step-by-step.\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to. The first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty. This is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\n\nggplot(data = penguins)\n\n\n\n\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot. The mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes. For now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\nThe following plot shows the result of adding these mappings.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot.\nTo do so, we need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_. People often describe plots by the type of geom that the plot uses. For example, bar charts use bar geoms (geom_bar()), line charts use line geoms (geom_line()), boxplots use boxplot geoms (geom_boxplot()), scatterplots use point geoms (geom_point()), and so on.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we have something that looks like what we might think of as a “scatterplot”. It doesn’t yet match our “ultimate goal” plot.\nWe’re seeing the warning message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values.\nLet’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = “lm”.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory: title adds a title and subtitle adds a subtitle to the plot. Other arguments match the aesthetic mappings, x is the x-axis label, y is the y-axis label, and color and shape define the label for the legend. In addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\nAs we move on from these introductory sections, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning:\nExpression 1:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping, in the remainder of the book, we won’t supply those names. That saves typing, and, by reducing the amount of extra text, makes it easier to see what’s different between plots.\nExpression 2:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\nWe can also use the pipe, |&gt;, which will allow you to create that plot.\nExpression 3:\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "chapter_1.html#creating-a-ggplot",
    "href": "chapter_1.html#creating-a-ggplot",
    "title": "1  Chapter 1",
    "section": "1.1 Creating a ggplot",
    "text": "1.1 Creating a ggplot\nLet’s recreate this plot step-by-step.\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to. The first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty. This is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\n\nggplot(data = penguins)\n\n\n\n\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot. The mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes. For now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\nThe following plot shows the result of adding these mappings.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot.\nTo do so, we need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_. People often describe plots by the type of geom that the plot uses. For example, bar charts use bar geoms (geom_bar()), line charts use line geoms (geom_line()), boxplots use boxplot geoms (geom_boxplot()), scatterplots use point geoms (geom_point()), and so on.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow we have something that looks like what we might think of as a “scatterplot”. It doesn’t yet match our “ultimate goal” plot.\nWe’re seeing the warning message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values.\nLet’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = “lm”.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory: title adds a title and subtitle adds a subtitle to the plot. Other arguments match the aesthetic mappings, x is the x-axis label, y is the y-axis label, and color and shape define the label for the legend. In addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body mass and flipper length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper length (mm)\", y = \"Body mass (g)\",\n    color = \"Species\", shape = \"Species\"\n  ) +\n  scale_color_colorblind()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Economics",
    "section": "",
    "text": "Preface\nWelcome to the captivating world where data science intersects with economics! In an era defined by vast amounts of data and an increasing reliance on data-driven decision-making, the synergy between these two fields has become paramount. The fusion of data science and economics has brought about a paradigm shift, enabling us to unravel hidden patterns, make precise predictions, and unlock unprecedented insights into the workings of our economic systems.\nThis course, “Data Science and Economics,” is designed to be a comprehensive exploration of the powerful union between these two disciplines. It aims to equip you with the fundamental knowledge and practical skills necessary to leverage data science techniques for economic analysis, policy formulation, and business strategy. Whether you are an aspiring economist, a data enthusiast seeking to enter the world of economics, or a seasoned professional looking to expand your analytical toolkit, this course will serve as your guide to navigate the intricate terrain of data-driven economic analysis.\nAs you embark on this educational journey, you will discover the intrinsic relationship between data science and economics. You will delve into the foundations of economics, gaining a deep understanding of macroeconomics, microeconomics, and the principles that shape our economic landscape. Simultaneously, you will acquire a solid grasp of essential data science concepts, such as data collection, cleaning, visualization, statistical analysis, and machine learning. These interconnected disciplines will empower you to harness the full potential of data to tackle complex economic problems, explore market dynamics, forecast trends, and inform evidence-based policy decisions.\nThroughout this course, you will be exposed to real-world case studies, engaging exercises, and hands-on projects that bridge the gap between theory and practice. You will learn how to effectively leverage economic data sets, analyze their nuances, and extract meaningful insights to answer critical questions. Moreover, you will acquire the skills to communicate your findings persuasively, presenting economic analyses in a concise and compelling manner to stakeholders and decision-makers.\nIt is important to note that this course is not only focused on mastering technical tools and methodologies. It also emphasizes the ethical considerations and responsible use of data science in the realm of economics. As we navigate the ever-expanding landscape of data, we must be cognizant of the potential biases, limitations, and social implications that arise from our analyses. By fostering a holistic understanding of the ethical dimensions, we aim to cultivate responsible data scientists who can leverage their skills for the betterment of society.\nIn conclusion, “Data Science and Economics” is an invitation to explore the captivating intersection of data science and economics, where the power of information merges with the complexities of economic systems. By immersing yourself in this course, you will unlock a world of analytical possibilities and develop a versatile skill set that will shape the future of economic analysis. We hope that this journey will inspire you to embrace the boundless opportunities presented by the fusion of data science and economics, empowering you to make informed decisions, unravel economic mysteries, and drive positive change in the world around us."
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "2  Data types and formats",
    "section": "",
    "text": "Most of the time, we will load data from an external file.\n\nCSV or comma-separated values files represent tabular data. This is conceptually similar to a spreadsheet of data values like those generated by Microsoft Excel or Google Spreadsheet. Each observation is separated by line breaks and each field within the observation is separa"
  },
  {
    "objectID": "chapter_2.html#ggplot2-calls",
    "href": "chapter_2.html#ggplot2-calls",
    "title": "Chapter 2",
    "section": "",
    "text": "As we move on from these introductory sections, we’ll transition to a more concise expression of ggplot2 code. So far we’ve been very explicit, which is helpful when you are learning:\nExpression 1:\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping, in the remainder of the book, we won’t supply those names. That saves typing, and, by reducing the amount of extra text, makes it easier to see what’s different between plots.\nExpression 2:\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\nWe can also use the pipe, |&gt;, which will allow you to create that plot.\nExpression 3:\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "3  Chapter 3",
    "section": "",
    "text": "4 Difference in Differences\npacman::p_load(tidyverse,\n               knitr)"
  },
  {
    "objectID": "chapter_3.html#wide-and-long-data-formats",
    "href": "chapter_3.html#wide-and-long-data-formats",
    "title": "3  Difference in Differences",
    "section": "3.3 Wide and long data formats",
    "text": "3.3 Wide and long data formats\nWide data set:\n\ncity_wide_df &lt;- read.csv(\"data/city_wide.csv\")\ncity_wide_df %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling()\n\n\n\n\ncity\npop_2000\npop_2010\npop_2020\n\n\n\n\nBerlin\n3.38\n3.45\n3.56\n\n\nRome\n3.70\n3.96\n4.26\n\n\nParis\n9.74\n10.46\n11.01\n\n\nLondon\n7.27\n8.04\n9.30\n\n\n\n\n\n\n\n\ncity_long_df &lt;- read.csv(\"data/city_long.csv\")\ncity_long_df %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling()\n\n\n\n\ncity\nyear\npop\n\n\n\n\nBerlin\n2000\n3.38\n\n\nBerlin\n2010\n3.45\n\n\nBerlin\n2020\n3.56\n\n\nRome\n2000\n3.70\n\n\nRome\n2010\n3.96\n\n\nRome\n2020\n4.26\n\n\nParis\n2000\n9.74\n\n\nParis\n2010\n10.46\n\n\nParis\n2020\n11.01\n\n\nLondon\n2000\n7.27\n\n\nLondon\n2010\n8.04\n\n\nLondon\n2020\n9.30\n\n\n\n\n\n\n\nConvert “city_wide_df” to long data set\n\ncity_wide_df %&gt;%\n  tidyr::pivot_longer(\n    cols = c(pop_2000, pop_2010, pop_2020), # -city, !city, starts_with(pop_), etc... would also work\n    names_to = \"year\", # where do we want the names of the columns to go? (year)\n    names_prefix = \"pop_\", # names_prefix removes matching text from the start of each variable name (not always necessary)\n    values_to = \"pop\" # where do we want the values in the columns to go? (pop)\n  )\n\n# A tibble: 12 × 3\n   city      year    pop\n   &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n 1 \"Berlin \" 2000   3.38\n 2 \"Berlin \" 2010   3.45\n 3 \"Berlin \" 2020   3.56\n 4 \"Rome \"   2000   3.7 \n 5 \"Rome \"   2010   3.96\n 6 \"Rome \"   2020   4.26\n 7 \"Paris \"  2000   9.74\n 8 \"Paris \"  2010  10.5 \n 9 \"Paris \"  2020  11.0 \n10 \"London \" 2000   7.27\n11 \"London \" 2010   8.04\n12 \"London \" 2020   9.3 \n\n\n\n3.3.1 Why do we care about the data format\nIn some instances long format datasets are required for advanced statistical analysis and graphing. For example, if we wanted to run the regression formulation of the difference in differences model, we would need to input our data in long format. Furthermore, having our data in long format is very useful when plotting. Packages such as ggplot2, expect that your data will be in long form for the most part.\n\nsoda_tax_df &lt;- read.csv(\"data/soda_tax_df.csv\")\n\nAre this wide or long data\n\nsoda_tax_df %&gt;% head(10)\n\n   id     district treatment   pre_tax  post_tax\n1   1 Snake Lounge         0 1687.6438 1705.5791\n2   2 Snake Lounge         0  427.2953  438.2526\n3   3 Snake Lounge         0  566.4693  559.6664\n4   4 Snake Lounge         0  606.9294  623.9057\n5   5 Snake Lounge         0  572.6402  606.8654\n6   6 Snake Lounge         0  496.0813  501.8001\n7   7 Snake Lounge         0  658.7283  670.4385\n8   8 Snake Lounge         0  497.8011  522.3219\n9   9 Snake Lounge         0  814.7636  846.2049\n10 10 Snake Lounge         0  502.5929  509.9525\n\n\nOur soda_tax_df is in wide format. We can convert our data to a long format to render the time and treatment dummy variables and save is to the soda_tax_df_long.\nWe will utilize the pivot_longer() function from tidyr to format our data frame.\n\nsoda_tax_df_long &lt;- \n  soda_tax_df %&gt;% # the wide format df\n  tidyr::pivot_longer(cols = c(pre_tax, post_tax), # both contain information about soda drank at two points in time\n                      names_to = \"period\", # grab the names of pre and post and save them to period\n                      values_to = \"soda_drank\") %&gt;% # grab values from pre and post and put them in soda_drank\n  dplyr::mutate(after_tax = ifelse(period == \"post_tax\", 1, 0)) # create dummy for period\n\nhead(soda_tax_df_long, 10)\n\n# A tibble: 10 × 6\n      id district     treatment period   soda_drank after_tax\n   &lt;int&gt; &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n 1     1 Snake Lounge         0 pre_tax       1688.         0\n 2     1 Snake Lounge         0 post_tax      1706.         1\n 3     2 Snake Lounge         0 pre_tax        427.         0\n 4     2 Snake Lounge         0 post_tax       438.         1\n 5     3 Snake Lounge         0 pre_tax        566.         0\n 6     3 Snake Lounge         0 post_tax       560.         1\n 7     4 Snake Lounge         0 pre_tax        607.         0\n 8     4 Snake Lounge         0 post_tax       624.         1\n 9     5 Snake Lounge         0 pre_tax        573.         0\n10     5 Snake Lounge         0 post_tax       607.         1\n\n\n\n\n3.3.2 Exploring our data\nWe can use our soda_tax_df to explore the distribution of soda consumption at different points in time.\nLet’s try first to look at the differences in the distribution only at the\n\n3.3.2.1 pre-tax time period:\n\nggplot(soda_tax_df, aes(x = pre_tax, fill = factor(treatment))) + \n  geom_density(alpha = 0.5) + # density plot with transparency (alpha = 0.5)\n  scale_fill_manual(name = \" \", # changes to fill dimension\n                     values = c(\"#a7a8aa\", \"#cc0055\"),\n                     labels = c(\"Control\", \"Treatment\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Distribution of soda consumption before the tax was imposed\",\n       x = \"Soda consumtion (oz)\",\n       y = \"Density\")\n\n\n\n\nLet’s look at the\n\n\n3.3.2.2 post-tax period:\n\nggplot(soda_tax_df, aes(x = post_tax, fill = factor(treatment))) + \n  geom_density(alpha = 0.5) + # density plot with transparency (alpha = 0.5)\n  scale_fill_manual(name = \" \", # changes to fill dimension\n                     values = c(\"#a7a8aa\", \"#cc0055\"),\n                     labels = c(\"Control\", \"Treatment\")) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Distribution of soda consumption after the tax was imposed\",\n       x = \"Soda consumtion (oz)\",\n       y = \"Density\")\n\n\n\n\nSince in our soda_tax_df_long we represent the time and soda consumption dimensions under the same columns, we can create even more complex graphs.\n\n\n3.3.2.3 grammar of graphs: Facets\nWe will use facet_grid() which forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\n\nsoda_tax_df_long %&gt;% \n  dplyr::mutate(period = ifelse(period == \"post_tax\", \"T1 - Post-tax\", \"T0 - Pre-tax\"), # create more meaningful labels\n                treatment = ifelse(treatment == 1, \"Treated (D=1)\", \"Untreated (D=0)\")) %&gt;%\n  dplyr::group_by(period, treatment) %&gt;% # group to extract means of each group at each time\n  dplyr::mutate(group_mean = mean(soda_drank)) %&gt;% # extract means of each group at each time\nggplot(., aes(x = soda_drank, fill = factor(treatment))) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(name = \" \", # changes to fill dimension\n                     values = c(\"#cc0055\", \"#a7a8aa\"),\n                     labels = c(\"Treatment\", \"Control\")) +\n  facet_grid(treatment~period) + # we specify the matrix (treatment and period)\n  geom_vline(aes(xintercept = group_mean), linetype = \"longdash\") + # add vertical line with the mean\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Soda consumed (oz)\",\n       y = \"Density\")\n\n\n\n\n\n\n\n3.3.3 Modeling and estimating\nSo far we have ignored time in our estimations. Difference-in-differences compares the changes in outcomes over time between units under different treatment states. This allows us to correct for any differences between the treatment and comparison groups that are constant over time assuming that the trends in time are parallel.\n\n3.3.3.1 Calculating without time\nIf we did not have the pre_tax baseline measure, we would likely utilize the post_tax to explore the average effect on the treated. In this case, we would model this as:\n\nafter_model &lt;- lm(post_tax ~ treatment, data = soda_tax_df)\nstargazer(after_model, type = \"text\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                             post_tax          \n-----------------------------------------------\ntreatment                   -146.918***        \n                              (3.798)          \n                                               \nConstant                    523.273***         \n                              (2.686)          \n                                               \n-----------------------------------------------\nObservations                   7,500           \nR2                             0.166           \nAdjusted R2                    0.166           \nResidual Std. Error     164.465 (df = 7498)    \nF Statistic         1,496.245*** (df = 1; 7498)\n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nWe could read this result substantively as: those who lived in districts were the tax was implemented consumed on average 146.9 ounces less of sugar-added drinks per week compared to those who lived in districts were the tax was not put in place. This calculation would give us a comparison of the treatment and control groups after treatment.\nWe would have to think carefully about possible factors that could differentiate our treatment and control groups. We use a treatment indicator based on the districts where the measure was able to be implemented. Treatment was not fully randomly assigned, so there may be lots of potential confounders that create baseline differences in the scores for those living in Old Eagleton compared to those in Snake Lounge, which also affect the after-treatment comparisons.\nIf we think about the mechanics behind this naive calculation, we are just comparing the average observed outcomes for those treated and not treated after the tax was imposed:\n\n\n\nTreatment\nAverage after tax\n\n\n\n\n0\n523.2726\n\n\n1\n376.3548\n\n\n\n\nggplot(soda_tax_df, aes(x = post_tax, fill = factor(treatment))) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(name = \" \", # changes to fill dimension\n                     values = c(\"#a7a8aa\", \"#cc0055\"),\n                     labels = c(\"Control\", \"Treatment\")) +\n  geom_vline(xintercept = 523.27, linetype = \"longdash\", color = \"#a7a8aa\") + #avg for the untreated\n  geom_vline(xintercept = 376.35, linetype = \"longdash\", color = \"#cc0055\") + #avg for the treated\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  labs(title = \"Distribution of soda consumption after the tax was imposed\",\n       x = \"Soda consumtion (oz)\",\n       y = \"Density\")\n\n\n\n\n\n\n3.3.3.2 Including the time dimension (Manual extraction of the DiD estimate)\n\n\n\nTreatment\nPre-tax\nPost-tax\nDifference\n\n\n\n\n1\n511.13\n376.35\n-134.78\n\n\n0\n508.31\n523.27\n14.97\n\n\n\nWe can just manually substract:\n\\[ DiD = -134.79 -14.97 = -149.76\\]\n\n\n3.3.3.3 Including the time dimension\nWe can introduce the time component to our calculation by incorporating the pre-treatment levels of sugar-added drink consumption, which gives us the diff-in-diff estimand. We could calculate this in a fairly straightforward manner by creating a variable assessing the change in our wide format data frame:\n\nchange: The difference in sugar-added drink consuption between post- and pre-tax\n\n\nsoda_tax_df &lt;- soda_tax_df %&gt;%\n  dplyr::mutate(change = post_tax - pre_tax) #simple substraction\n\ndid_model &lt;- lm(change ~ treatment, data = soda_tax_df)\nstargazer(did_model, after_model, type = \"text\")\n\n\n============================================================\n                                    Dependent variable:     \n                                ----------------------------\n                                    change        post_tax  \n                                      (1)           (2)     \n------------------------------------------------------------\ntreatment                         -149.744***   -146.918*** \n                                    (0.246)       (3.798)   \n                                                            \nConstant                           14.967***     523.273*** \n                                    (0.174)       (2.686)   \n                                                            \n------------------------------------------------------------\nObservations                         7,500         7,500    \nR2                                   0.980         0.166    \nAdjusted R2                          0.980         0.166    \nResidual Std. Error (df = 7498)     10.671        164.465   \nF Statistic (df = 1; 7498)      369,242.400***  1,496.245***\n============================================================\nNote:                            *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nWe could read this result substantively as: those who lived in districts were the tax was implemented consumed on average 149.7 ounces less of sugar-added drinks per week compared to those who lived in districts were the tax was not put in place. This calculation would give us the change, or difference, in sugar-added drink consumption for treatment and control groups.\nTo believe the results of our did_model, we would need to believe that there are parallel trends between the two groups.\n\n\n3.3.3.4 Including the time dimension (Regression formulation of the DiD model)\nWe can re-format our data to gather our diff-in-diff estimate\n\\[ Y_{it} = ......\\]\nFor this calculation we need our data in long format to use the time and treatment dummy variables. We can see that under our long format, we have two entries for every individual.\n\ndid_long &lt;- lm(soda_drank ~ treatment + after_tax + treatment*after_tax, data = soda_tax_df_long) #running our model\n\ndid_long_clustered_se &lt;- coeftest(did_long, vcov=vcovHC(did_long,type=\"HC0\",cluster=\"district\")) #clustering out standard errors at the district level\n\nstargazer::stargazer(did_long_clustered_se, type = \"text\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                                               \n-----------------------------------------------\ntreatment                      2.827           \n                              (3.799)          \n                                               \nafter_tax                    14.967***         \n                              (3.836)          \n                                               \ntreatment:after_tax         -149.744***        \n                              (5.372)          \n                                               \nConstant                    508.306***         \n                              (2.708)          \n                                               \n===============================================\n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\nsoda_tax_df_long %&gt;% \n  dplyr::mutate(period = ifelse(period == \"post_tax\", \"T1 - Post-tax\", \"T0 - Pre-tax\"), # create more meaningful labels\n                treatment = ifelse(treatment == 1, \"Treated (D=1)\", \"Untreated (D=0)\")) %&gt;%\n  dplyr::group_by(period, treatment) %&gt;% # group to extract means of each group at each time\n  dplyr::mutate(group_mean = mean(soda_drank)) %&gt;% # extract means of each group at each time\nggplot(., aes(x = soda_drank, fill = factor(treatment))) +\n  geom_density(alpha = 0.5) +\n  scale_fill_manual(name = \" \", # changes to fill dimension\n                     values = c(\"#cc0055\", \"#a7a8aa\"),\n                     labels = c(\"Treatment\", \"Control\")) +\n  facet_grid(treatment~period) + # we specify the matrix (treatment and period)\n  geom_vline(aes(xintercept = group_mean), linetype = \"longdash\") + # add vertical line with the mean\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(x = \"Soda consumed (oz)\",\n       y = \"Density\")\n\n\n\n\n\nsoda_tax_df_long %&gt;%\n  dplyr::group_by(period, treatment) %&gt;% # group to extract means of each group at each time\n  dplyr::mutate(group_mean = mean(soda_drank)) %&gt;%\n  ggplot(aes(x = after_tax, y = group_mean, color = factor(treatment))) +\n  geom_point() +\n  geom_line(aes(x = after_tax, y = group_mean)) +\n  scale_x_continuous(breaks = c(0,1)) +\n  scale_color_manual(name = \" \", # changes to color dimension\n                     values = c(\"#a7a8aa\", \"#cc0055\"),\n                     labels = c(\"Control\", \"Treatment\")) +\n  labs(x = \"Time periods\", y = \"Ounces of soda drank per week\", color = \"Treatment group\")+\n  theme_minimal() \n\n\n\n\n\nDrafting some brief recommedations\n\nBased on your analysis of the data at hand, you decide to recommend that the tax measure should move forward in the rest of Pawnee. You state that it is a very good example of a pigouvian tax, which captures the negative externalities not included in the market price of sugar-added drinks. The findings suggest that the tax reduced the weekly sugar-added drink consumption by about 150 luquid ounces (almost 4.5 liters)."
  },
  {
    "objectID": "chapter_4.html#basic-2d-vann-diagram",
    "href": "chapter_4.html#basic-2d-vann-diagram",
    "title": "4  Create simple vann diagram",
    "section": "4.1 Basic 2D Vann diagram:",
    "text": "4.1 Basic 2D Vann diagram:\n\nx &lt;- list(A = 1:5, B = 2:7)\nggVennDiagram(x)"
  },
  {
    "objectID": "chapter_4.html#d-venn-diagram",
    "href": "chapter_4.html#d-venn-diagram",
    "title": "4  Create simple vann diagram",
    "section": "4.2 3D Venn diagram:",
    "text": "4.2 3D Venn diagram:\nIf you pass a list with three elements you will get some Venn diagram like the following\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10)\nggVennDiagram(x)"
  },
  {
    "objectID": "chapter_4.html#changing-the-colors-of-the-diagram",
    "href": "chapter_4.html#changing-the-colors-of-the-diagram",
    "title": "4  Create simple vann diagram",
    "section": "4.3 Changing the colors of the diagram",
    "text": "4.3 Changing the colors of the diagram\n\nAs ggVennDiagram is based on ggplot2 you can add more layers or override the ones existing. In order to change the colors use scale_fill_gradient as in the following example:\n\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10)\nggVennDiagram(x) + \n  scale_fill_gradient(low = \"#F4FAFE\", high = \"#4981BF\") \n\n\n\n\n\nThe border can also be customized but making use of color argument. The line width and line style can be changed with lwd and lty, respectively."
  },
  {
    "objectID": "chapter_4.html#labels-and-group-names",
    "href": "chapter_4.html#labels-and-group-names",
    "title": "4  Create simple vann diagram",
    "section": "4.4 Labels and group names",
    "text": "4.4 Labels and group names\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10)\nggVennDiagram(x, category.names = c(\"A\",\n                                    \"B\",\n                                    \"C\"))\n\n\n\n\n\nLabels with percentages\n\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10)\nggVennDiagram(x, \n              label = \"percent\",\n              category.names = c(\"A\",\n                                    \"B\",\n                                    \"C\")) \n\n\n\n\n\nRemove the labels\n\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10)\nggVennDiagram(x, \n              label = \"none\",\n              category.names = c(\"A\",\n                                    \"B\",\n                                    \"C\")) \n\n\n\n\n-Customize the legend\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10)\nggVennDiagram(x) + \n  guides(fill = guide_legend(title = \"Title\")) +\n  theme(legend.title = element_text(color = \"black\"),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "chapter_4.html#d-venn-diagram-1",
    "href": "chapter_4.html#d-venn-diagram-1",
    "title": "4  Create simple vann diagram",
    "section": "4.5 4D Venn diagram",
    "text": "4.5 4D Venn diagram\nYou can also pass a list with up to four different elements.\n\nx &lt;- list(A = 1:5, B = 2:7, C = 5:10, D = 8:15)\nggVennDiagram(x)"
  },
  {
    "objectID": "chapter_5.html",
    "href": "chapter_5.html",
    "title": "5  Regression models",
    "section": "",
    "text": "6 Chapter 5: Multiple Regression Analysis: OLS Asymptotics"
  },
  {
    "objectID": "chapter_5.html#chapter-9-more-on-specification-and-data-issues",
    "href": "chapter_5.html#chapter-9-more-on-specification-and-data-issues",
    "title": "5  Regression models",
    "section": "5.9 Chapter 9: More on Specification and Data Issues",
    "text": "5.9 Chapter 9: More on Specification and Data Issues\n\n5.9.1 Example 9.8: R&D Intensity and Firm Size\n\\[rdintens = \\beta_0 + \\beta_1sales + \\beta_2profmarg + \\mu\\]\nFrom Businessweek R&D Scoreboard, October 25, 1991. Load the data and estimate the model.\n\nall_rdchem &lt;- lm(rdintens ~ sales + profmarg, data = rdchem)\n\nPlotting the data reveals the outlier on the far right of the plot, which will skew the results of our model.\n\n\n\n\n\nSo, we can estimate the model without that data point to gain a better understanding of how sales and profmarg describe rdintens for most firms. We can use the subset argument of the linear model function to indicate that we only want to estimate the model using data that is less than the highest sales.\n\nsmallest_rdchem &lt;- lm(rdintens ~ sales + profmarg, data = rdchem, \n                      subset = (sales &lt; max(sales)))\n\nThe table below compares the results of both models side by side. By removing the outlier firm, \\(sales\\) become a more significant determination of R&D expenditures.\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nrdintens\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nsales\n\n\n0.00005 (0.00004)\n\n\n0.00019** (0.00008)\n\n\n\n\nprofmarg\n\n\n0.04462 (0.04618)\n\n\n0.04784 (0.04448)\n\n\n\n\nConstant\n\n\n2.62526*** (0.58553)\n\n\n2.29685*** (0.59180)\n\n\n\n\n\n\n\n\nObservations\n\n\n32\n\n\n31\n\n\n\n\nR2\n\n\n0.07612\n\n\n0.17281\n\n\n\n\nAdjusted R2\n\n\n0.01240\n\n\n0.11372\n\n\n\n\nResidual Std. Error\n\n\n1.86205 (df = 29)\n\n\n1.79218 (df = 28)\n\n\n\n\nF Statistic\n\n\n1.19465 (df = 2; 29)\n\n\n2.92476* (df = 2; 28)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#data",
    "href": "chapter_5.html#data",
    "title": "5  Regression models",
    "section": "5.1 Data",
    "text": "5.1 Data\n\nearns &lt;- read.csv(\"data/earns.csv\")\ngpa1 &lt;- read.csv(\"data/gpa1.csv\")\nhprice1 &lt;- read.csv(\"data/hprice1.csv\")\nhprice2 &lt;- read.csv(\"data/hprice2.csv\")\nhprice3 &lt;- read.csv(\"data/hprice3.csv\")\njtrain &lt;- read.csv(\"data/jtrain.csv\")\nnyse &lt;- read.csv(\"data/nyse.csv\")\nphillips &lt;- read.csv(\"data/phillips.csv\")\nrdchem &lt;- read.csv(\"data/rdchem.csv\")\ntraffic1 &lt;- read.csv(\"data/traffic1.csv\")\nwage1 &lt;- read.csv(\"data/wage1.csv\")"
  },
  {
    "objectID": "chapter_5.html#the-simple-regression-model",
    "href": "chapter_5.html#the-simple-regression-model",
    "title": "5  Regression models",
    "section": "5.2 The Simple Regression Model",
    "text": "5.2 The Simple Regression Model\n\n5.2.1 Example 2.10: A Log Wage Equation\nLoad the wage1 data and check out the documentation. The documentation indicates these are data from the 1976 Current Population Survey, collected by Henry Farber when he and Wooldridge were colleagues at MIT in 1988.\n\\(educ\\): years of education\n\\(wage\\): average hourly earnings\n\\(lwage\\): log of the average hourly earnings\nFirst, make a scatter-plot of the two variables and look for possible patterns in the relationship between them.\n\n\n\n\n\nIt appears that on average, more years of education, leads to higher wages.\nThe example in the text is interested in the return to another year of education, or what the percentage change in wages one might expect for each additional year of education. To do so, one must use the \\(log(\\)wage\\()\\). This has already been computed in the data set and is defined as lwage.\nThe textbook provides excellent discussions around these topics, so please consult it.\nBuild a linear model to estimate the relationship between the log of wage (lwage) and education (educ).\n\\[\\widehat{log(wage)} = \\beta_0 + \\beta_1educ\\]\n\nlog_wage_model &lt;- lm(lwage ~ educ, data = wage1)\n\nPrint the summary of the results.\n\nsummary(log_wage_model)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlwage\n\n\n\n\n\n\n\n\neduc\n\n\n0.08274*** (0.00757)\n\n\n\n\nConstant\n\n\n0.58377*** (0.09734)\n\n\n\n\n\n\n\n\nObservations\n\n\n526\n\n\n\n\nR2\n\n\n0.18581\n\n\n\n\nAdjusted R2\n\n\n0.18425\n\n\n\n\nResidual Std. Error\n\n\n0.48008 (df = 524)\n\n\n\n\nF Statistic\n\n\n119.58160*** (df = 1; 524)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nPlot the \\(log(\\)wage\\()\\) vs educ. The blue line represents the least squares fit."
  },
  {
    "objectID": "chapter_5.html#chapter-3-multiple-regression-analysis-estimation",
    "href": "chapter_5.html#chapter-3-multiple-regression-analysis-estimation",
    "title": "5  Regression models",
    "section": "5.3 Chapter 3: Multiple Regression Analysis: Estimation",
    "text": "5.3 Chapter 3: Multiple Regression Analysis: Estimation\n\n5.3.1 Example 3.2: Hourly Wage Equation\nCheck the documentation for variable information\n\\(lwage\\): log of the average hourly earnings\n\\(educ\\): years of education\n\\(exper\\): years of potential experience\n\\(tenutre\\): years with current employer\nPlot the variables against lwage and compare their distributions and slope (\\(\\beta\\)) of the simple regression lines.\n\n\n\n\n\nEstimate the model regressing educ, exper, and tenure against log(wage).\n\\[\\widehat{log(wage)} = \\beta_0 + \\beta_1educ + \\beta_3exper + \\beta_4tenure\\]\n\nhourly_wage_model &lt;- lm(lwage ~ educ + exper + tenure, data = wage1)\n\nPrint the estimated model coefficients:\n\ncoefficients(hourly_wage_model)\n\n\n\n\n\n\n\nCoefficients\n\n\n\n\n(Intercept)\n0.2844\n\n\neduc\n0.0920\n\n\nexper\n0.0041\n\n\ntenure\n0.0221\n\n\n\n\n\n\n\nPlot the coefficients, representing percentage impact of each variable on \\(log(\\)wage\\()\\) for a quick comparison."
  },
  {
    "objectID": "chapter_5.html#chapter-4-multiple-regression-analysis-inference",
    "href": "chapter_5.html#chapter-4-multiple-regression-analysis-inference",
    "title": "5  Regression models",
    "section": "5.4 Chapter 4: Multiple Regression Analysis: Inference",
    "text": "5.4 Chapter 4: Multiple Regression Analysis: Inference\n\n5.4.1 Example 4.1 Hourly Wage Equation\nUsing the same model estimated in example: 3.2, examine and compare the standard errors associated with each coefficient. Like the textbook, these are contained in parenthesis next to each associated coefficient.\n\nsummary(hourly_wage_model)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlwage\n\n\n\n\n\n\n\n\neduc\n\n\n0.09203*** (0.00733)\n\n\n\n\nexper\n\n\n0.00412** (0.00172)\n\n\n\n\ntenure\n\n\n0.02207*** (0.00309)\n\n\n\n\nConstant\n\n\n0.28436*** (0.10419)\n\n\n\n\n\n\n\n\nObservations\n\n\n526\n\n\n\n\nR2\n\n\n0.31601\n\n\n\n\nAdjusted R2\n\n\n0.31208\n\n\n\n\nResidual Std. Error\n\n\n0.44086 (df = 522)\n\n\n\n\nF Statistic\n\n\n80.39092*** (df = 3; 522)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nFor the years of experience variable, or exper, use coefficient and Standard Error to compute the \\(t\\) statistic:\n\\[t_{exper} = \\frac{0.004121}{0.001723} = 2.391\\]\nFortunately, R includes \\(t\\) statistics in the summary of model diagnostics.\n\nsummary(hourly_wage_model)$coefficients\n\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n0.28436\n0.10419\n2.72923\n0.00656\n\n\neduc\n0.09203\n0.00733\n12.55525\n0.00000\n\n\nexper\n0.00412\n0.00172\n2.39144\n0.01714\n\n\ntenure\n0.02207\n0.00309\n7.13307\n0.00000\n\n\n\n\n\n\n\nPlot the \\(t\\) statistics for a visual comparison:\n\n\n\n\n\n\n\n5.4.2 Example 4.7 Effect of Job Training on Firm Scrap Rates\nLoad the jtrain data set.\nFrom H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), Are Training Subsidies Effective? The Michigan Experience, Industrial and Labor Relations Review 46, 625-636. The authors kindly provided the data.\n\\(year:\\) 1987, 1988, or 1989\n\\(union:\\) =1 if unionized\n\\(lscrap:\\) Log(scrap rate per 100 items)\n\\(hrsemp:\\) (total hours training) / (total employees trained)\n\\(lsales:\\) Log(annual sales, $)\n\\(lemploy:\\) Log(umber of employees at plant)\nFirst, use the subset function and it’s argument by the same name to return observations which occurred in 1987 and are not union. At the same time, use the select argument to return only the variables of interest for this problem.\n\njtrain_subset &lt;- subset(jtrain, subset = (year == 1987 & union == 0),\n                        select = c(year, union, lscrap, hrsemp, lsales, lemploy))\n\nNext, test for missing values. One can “eyeball” these with R Studio’s View function, but a more precise approach combines the sum and is.na functions to return the total number of observations equal to NA.\n\nsum(is.na(jtrain_subset))\n\n[1] 156\n\n\nWhile R’s lm function will automatically remove missing NA values, eliminating these manually will produce more clearly proportioned graphs for exploratory analysis. Call the na.omit function to remove all missing values and assign the new data.frame object the name jtrain_clean.\n\njtrain_clean &lt;- na.omit(jtrain_subset)\n\nUse jtrain_clean to plot the variables of interest against lscrap. Visually observe the respective distributions for each variable, and compare the slope (\\(\\beta\\)) of the simple regression lines.\n\n\n\n\n\nNow create the linear model regressing hrsemp(total hours training/total employees trained), lsales(log of annual sales), and lemploy(the log of the number of the employees), against lscrap(the log of the scrape rate).\n\\[lscrap = \\alpha + \\beta_1 hrsemp + \\beta_2 lsales + \\beta_3 lemploy\\]\n\nlinear_model &lt;- lm(lscrap ~ hrsemp + lsales + lemploy, data = jtrain_clean)\n\nFinally, print the complete summary diagnostics of the model.\n\nsummary(linear_model)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlscrap\n\n\n\n\n\n\n\n\nhrsemp\n\n\n-0.02927 (0.02280)\n\n\n\n\nlsales\n\n\n-0.96203** (0.45252)\n\n\n\n\nlemploy\n\n\n0.76147* (0.40743)\n\n\n\n\nConstant\n\n\n12.45837** (5.68677)\n\n\n\n\n\n\n\n\nObservations\n\n\n29\n\n\n\n\nR2\n\n\n0.26243\n\n\n\n\nAdjusted R2\n\n\n0.17392\n\n\n\n\nResidual Std. Error\n\n\n1.37604 (df = 25)\n\n\n\n\nF Statistic\n\n\n2.96504* (df = 3; 25)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-6-multiple-regression-further-issues",
    "href": "chapter_5.html#chapter-6-multiple-regression-further-issues",
    "title": "5  Regression models",
    "section": "5.6 Chapter 6: Multiple Regression: Further Issues",
    "text": "5.6 Chapter 6: Multiple Regression: Further Issues\n\n5.6.1 Example 6.1: Effects of Pollution on Housing Prices, standardized.\nLoad the hprice2 data and view the documentation.\nData from Hedonic Housing Prices and the Demand for Clean Air, by Harrison, D. and D.L.Rubinfeld, Journal of Environmental Economics and Management 5, 81-102. Diego Garcia, a former Ph.D. student in economics at MIT, kindly provided these data, which he obtained from the book Regression Diagnostics: Identifying Influential Data and Sources of Collinearity, by D.A. Belsey, E. Kuh, and R. Welsch, 1990. New York: Wiley.\n\\(price\\): median housing price.\n\\(nox\\): Nitrous Oxide concentration; parts per million.\n\\(crime\\): number of reported crimes per capita.\n\\(rooms\\): average number of rooms in houses in the community.\n\\(dist\\): weighted distance of the community to 5 employment centers.\n\\(stratio\\): average student-teacher ratio of schools in the community.\n\\[price = \\beta_0 + \\beta_1nox + \\beta_2crime + \\beta_3rooms + \\beta_4dist + \\beta_5stratio + \\mu\\]\nEstimate the usual lm model.\n\nhousing_level &lt;- lm(price ~ nox + crime + rooms + dist + stratio, data = hprice2)\n\nEstimate the same model, but standardized coefficients by wrapping each variable with R’s scale function:\n\\[\\widehat{zprice} = \\beta_1znox + \\beta_2zcrime + \\beta_3zrooms + \\beta_4zdist + \\beta_5zstratio\\]\n\nhousing_standardized &lt;- lm(scale(price) ~ 0 + scale(nox) + scale(crime) + scale(rooms) + scale(dist) + scale(stratio), data = hprice2)\n\nCompare results, and observe\n\nsummary(housing_level)\nsummary(housing_standardized)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nprice\n\n\nscale(price)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nnox\n\n\n-2,706.43300*** (354.08690)\n\n\n\n\n\n\ncrime\n\n\n-153.60100*** (32.92883)\n\n\n\n\n\n\nrooms\n\n\n6,735.49800*** (393.60370)\n\n\n\n\n\n\ndist\n\n\n-1,026.80600*** (188.10790)\n\n\n\n\n\n\nstratio\n\n\n-1,149.20400*** (127.42870)\n\n\n\n\n\n\nscale(nox)\n\n\n\n\n-0.34045*** (0.04450)\n\n\n\n\nscale(crime)\n\n\n\n\n-0.14328*** (0.03069)\n\n\n\n\nscale(rooms)\n\n\n\n\n0.51389*** (0.03000)\n\n\n\n\nscale(dist)\n\n\n\n\n-0.23484*** (0.04298)\n\n\n\n\nscale(stratio)\n\n\n\n\n-0.27028*** (0.02994)\n\n\n\n\nConstant\n\n\n20,871.13000*** (5,054.59900)\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n506\n\n\n506\n\n\n\n\nR2\n\n\n0.63567\n\n\n0.63567\n\n\n\n\nAdjusted R2\n\n\n0.63202\n\n\n0.63203\n\n\n\n\nResidual Std. Error\n\n\n5,586.19800 (df = 500)\n\n\n0.60601 (df = 501)\n\n\n\n\nF Statistic\n\n\n174.47330*** (df = 5; 500)\n\n\n174.82220*** (df = 5; 501)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\n5.6.2 Example 6.2: Effects of Pollution on Housing Prices, Quadratic Interactive Term\nModify the housing model from example 4.5, adding a quadratic term in rooms:\n\\[log(price) = \\beta_0 + \\beta_1log(nox) + \\beta_2log(dist) + \\beta_3rooms + \\beta_4rooms^2 + \\beta_5stratio + \\mu\\]\n\nhousing_model_4.5 &lt;- lm(lprice ~ lnox + log(dist) + rooms + stratio, data = hprice2)\nhousing_model_6.2 &lt;- lm(lprice ~ lnox + log(dist) + rooms + I(rooms^2) + stratio, \n                        data = hprice2)\n\nCompare the results with the model from example 6.1.\n\nsummary(housing_model_4.5)\nsummary(housing_model_6.2)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlprice\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nlnox\n\n\n-0.95354*** (0.11674)\n\n\n-0.90168*** (0.11469)\n\n\n\n\nlog(dist)\n\n\n-0.13434*** (0.04310)\n\n\n-0.08678** (0.04328)\n\n\n\n\nrooms\n\n\n0.25453*** (0.01853)\n\n\n-0.54511*** (0.16545)\n\n\n\n\nI(rooms2)\n\n\n\n\n0.06226*** (0.01280)\n\n\n\n\nstratio\n\n\n-0.05245*** (0.00590)\n\n\n-0.04759*** (0.00585)\n\n\n\n\nConstant\n\n\n11.08386*** (0.31811)\n\n\n13.38548*** (0.56647)\n\n\n\n\n\n\n\n\nObservations\n\n\n506\n\n\n506\n\n\n\n\nR2\n\n\n0.58403\n\n\n0.60281\n\n\n\n\nAdjusted R2\n\n\n0.58071\n\n\n0.59884\n\n\n\n\nResidual Std. Error\n\n\n0.26500 (df = 501)\n\n\n0.25921 (df = 500)\n\n\n\n\nF Statistic\n\n\n175.85520*** (df = 4; 501)\n\n\n151.77040*** (df = 5; 500)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nEstimate the minimum turning point at which the rooms interactive term changes from negative to positive.\n\\[x = \\frac{\\hat{\\beta_1}}{2\\hat{\\beta_2}}\\]\n\nbeta_1 &lt;- summary(housing_model_6.2)$coefficients[\"rooms\",1] \nbeta_2 &lt;- summary(housing_model_6.2)$coefficients[\"I(rooms^2)\",1]\nturning_point &lt;- abs(beta_1 / (2*beta_2))\nprint(turning_point)\n\n[1] 4.37763\n\n\nCompute the percent change across a range of average rooms. Include the smallest, turning point, and largest.\n\nRooms &lt;- c(min(hprice2$rooms), 4, turning_point, 5, 5.5, 6.45, 7.5, max(hprice2$rooms))\nPercent.Change &lt;- 100*(beta_1 + 2*beta_2*Rooms)\nkable(data.frame(Rooms, Percent.Change))\n\n\n\n\nRooms\nPercent.Change\n\n\n\n\n3.56000\n-10.181324\n\n\n4.00000\n-4.702338\n\n\n4.37763\n0.000000\n\n\n5.00000\n7.749903\n\n\n5.50000\n13.976023\n\n\n6.45000\n25.805651\n\n\n7.50000\n38.880503\n\n\n8.78000\n54.819367\n\n\n\n\n\n\n\nGraph the log of the selling price against the number of rooms. Superimpose a simple model as well as a quadratic model and examine the difference."
  },
  {
    "objectID": "chapter_5.html#chapter-7-multiple-regression-analysis-with-qualitative-information",
    "href": "chapter_5.html#chapter-7-multiple-regression-analysis-with-qualitative-information",
    "title": "5  Regression models",
    "section": "5.7 Chapter 7: Multiple Regression Analysis with Qualitative Information",
    "text": "5.7 Chapter 7: Multiple Regression Analysis with Qualitative Information\n\n5.7.1 Example 7.4: Housing Price Regression, Qualitative Binary variable\nThis time, use the hrprice1 data.\nData collected from the real estate pages of the Boston Globe during 1990. These are homes that sold in the Boston, MA area.\n\\(lprice:\\) Log(house price, $1000s)\n\\(llotsize:\\) Log(size of lot in square feet)\n\\(lsqrft:\\) Log(size of house in square feet)\n\\(bdrms:\\) number of bdrms\n\\(colonial:\\) =1 if home is colonial style\n\\[\\widehat{log(price)} = \\beta_0 + \\beta_1log(lotsize) + \\beta_2log(sqrft) + \\beta_3bdrms + \\beta_4colonial \\]\nEstimate the coefficients of the above linear model on the hprice data set.\n\nhousing_qualitative &lt;- lm(lprice ~ llotsize + lsqrft + bdrms + colonial, data = hprice1)\n\n\nsummary(housing_qualitative)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlprice\n\n\n\n\n\n\n\n\nllotsize\n\n\n0.16782*** (0.03818)\n\n\n\n\nlsqrft\n\n\n0.70719*** (0.09280)\n\n\n\n\nbdrms\n\n\n0.02683 (0.02872)\n\n\n\n\ncolonial\n\n\n0.05380 (0.04477)\n\n\n\n\nConstant\n\n\n-1.34959** (0.65104)\n\n\n\n\n\n\n\n\nObservations\n\n\n88\n\n\n\n\nR2\n\n\n0.64907\n\n\n\n\nAdjusted R2\n\n\n0.63216\n\n\n\n\nResidual Std. Error\n\n\n0.18412 (df = 83)\n\n\n\n\nF Statistic\n\n\n38.37846*** (df = 4; 83)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-8-heteroskedasticity",
    "href": "chapter_5.html#chapter-8-heteroskedasticity",
    "title": "5  Regression models",
    "section": "5.8 Chapter 8: Heteroskedasticity",
    "text": "5.8 Chapter 8: Heteroskedasticity\n\n5.8.1 Example 8.9: Determinants of Personal Computer Ownership\n\\[\\widehat{PC} = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + \\beta_3parcoll + \\beta_4colonial \\] Christopher Lemmon, a former MSU undergraduate, collected these data from a survey he took of MSU students in Fall 1994. Load gpa1 and create a new variable combining the fathcoll and mothcoll, into parcoll. This new column indicates if either parent went to college.\n\ngpa1$parcoll &lt;- as.integer(gpa1$fathcoll==1 | gpa1$mothcoll)\nGPA_OLS &lt;- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1)\n\nCalculate the weights and then pass them to the weights argument.\n\nweights &lt;- GPA_OLS$fitted.values * (1-GPA_OLS$fitted.values)\nGPA_WLS &lt;- lm(PC ~ hsGPA + ACT + parcoll, data = gpa1, weights = 1/weights)\n\nCompare the OLS and WLS model in the table below:\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nPC\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nhsGPA\n\n\n0.06539 (0.13726)\n\n\n0.03270 (0.12988)\n\n\n\n\nACT\n\n\n0.00056 (0.01550)\n\n\n0.00427 (0.01545)\n\n\n\n\nparcoll\n\n\n0.22105** (0.09296)\n\n\n0.21519** (0.08629)\n\n\n\n\nConstant\n\n\n-0.00043 (0.49054)\n\n\n0.02621 (0.47665)\n\n\n\n\n\n\n\n\nObservations\n\n\n141\n\n\n141\n\n\n\n\nR2\n\n\n0.04153\n\n\n0.04644\n\n\n\n\nAdjusted R2\n\n\n0.02054\n\n\n0.02556\n\n\n\n\nResidual Std. Error (df = 137)\n\n\n0.48599\n\n\n1.01624\n\n\n\n\nF Statistic (df = 3; 137)\n\n\n1.97851\n\n\n2.22404*\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-10-basic-regression-analysis-with-time-series-data",
    "href": "chapter_5.html#chapter-10-basic-regression-analysis-with-time-series-data",
    "title": "5  Regression models",
    "section": "5.10 Chapter 10: Basic Regression Analysis with Time Series Data",
    "text": "5.10 Chapter 10: Basic Regression Analysis with Time Series Data\n\n5.10.1 Example 10.2: Effects of Inflation and Deficits on Interest Rates\n\\[\\widehat{i3} = \\beta_0 + \\beta_1inf_t + \\beta_2def_t\\] Data from the Economic Report of the President, 2004, Tables B-64, B-73, and B-79.\n\n#data(\"intdef\") # load data\n# load eXtensible Time Series package.\n# xts is excellent for time series plots and \n# properly indexing time series.\n#library(xts) \n# create xts object from data.frame\n# First, index year as yearmon class of monthly data. \n# Note: I add 11/12 to set the month to December, end of year.\nindex &lt;- zoo::as.yearmon(intdef$year + 11/12)\n# Next, create the xts object, ordering by the index above.\nintdef.xts &lt;- xts(intdef[ ,-1], order.by = index) \n# extract 3-month Tbill, inflation, and deficit data\nintdef.xts &lt;- intdef.xts[ ,c(\"i3\", \"inf\", \"def\")] \n# rename with clearer names\ncolnames(intdef.xts) &lt;- c(\"Tbill3mo\", \"cpi\", \"deficit\") \n# plot the object, add a title, and place legend at top left.\nplot(x = intdef.xts, \n     main = \"Inflation, Deficits, and Interest Rates\",\n     legend.loc = \"topleft\")\n\n\n\n# Run a Linear regression model\ntbill_model &lt;- lm(Tbill3mo ~ cpi + deficit, data = intdef.xts)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nTbill3mo\n\n\n\n\n\n\n\n\ncpi\n\n\n0.60587*** (0.08213)\n\n\n\n\ndeficit\n\n\n0.51306*** (0.11838)\n\n\n\n\nConstant\n\n\n1.73327*** (0.43197)\n\n\n\n\n\n\n\n\nObservations\n\n\n56\n\n\n\n\nR2\n\n\n0.60207\n\n\n\n\nAdjusted R2\n\n\n0.58705\n\n\n\n\nResidual Std. Error\n\n\n1.84316 (df = 53)\n\n\n\n\nF Statistic\n\n\n40.09424*** (df = 2; 53)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nNow lets update the example with current data, pull from the Federal Reserve Economic Research (FRED) using the quantmod package. Other than the convenient API, the package also formats time series data into xts: eXtensible Time Series objects, which add many feature and benefits when working with time series.\n\n#library(quantmod)\n# Tbill, 3 month\ngetSymbols(\"TB3MS\", src = \"FRED\")\n\n[1] \"TB3MS\"\n\n# convert to annual observations and convert index to type `yearmon`.\nTB3MS &lt;- to.yearly(TB3MS, OHLC=FALSE, drop.time = TRUE)\nindex(TB3MS) &lt;- zoo::as.yearmon(index(TB3MS))\n# Inflation\ngetSymbols(\"FPCPITOTLZGUSA\", src = \"FRED\")\n\n[1] \"FPCPITOTLZGUSA\"\n\n# Convert the index to yearmon and shift FRED's Jan 1st to Dec\nindex(FPCPITOTLZGUSA) &lt;- zoo::as.yearmon(index(FPCPITOTLZGUSA)) + 11/12\n# Rename and update column names\ninflation &lt;- FPCPITOTLZGUSA\ncolnames(inflation) &lt;- \"inflation\"\n## Deficit, percent of GDP: Federal outlays - federal receipts\n # Download outlays\ngetSymbols(\"FYFRGDA188S\", src = \"FRED\")\n\n[1] \"FYFRGDA188S\"\n\n # Lets move the index from Jan 1st to Dec 30th/31st\nindex(FYFRGDA188S) &lt;- zoo::as.yearmon(index(FYFRGDA188S)) + 11/12\n # Rename and update column names\noutlays &lt;- FYFRGDA188S\ncolnames(outlays) &lt;- \"outlays\"\n # Download receipts\ngetSymbols(\"FYONGDA188S\", src = \"FRED\")\n\n[1] \"FYONGDA188S\"\n\n # Lets move the index from Jan 1st to Dec 30th/31st\nindex(FYONGDA188S) &lt;- zoo::as.yearmon(index(FYONGDA188S)) + 11/12\n # Rename and update column names\nreceipts &lt;- FYONGDA188S\ncolnames(receipts) &lt;- \"receipts\"\n\nNow that all data has been downloaded, we can calculate the deficit from the federal outlays and receipts data. Next, we will merge our new deficit variable with inflation and TB3MS variables. As these are all xts times series objects, the merge function will automatically key off each series time date index, insuring integrity and alignment among each observation and its respective date. Additionally, xts provides easy chart construction with its plot method.\n\n# create deficits from outlays - receipts\n# xts objects respect their indexing and outline the future\ndeficit &lt;- outlays - receipts\ncolnames(deficit) &lt;- \"deficit\"\n# Merge and remove leading and trailing NAs for a balanced data matrix\nintdef_updated &lt;- merge(TB3MS, inflation, deficit)\nintdef_updated &lt;- zoo::na.trim(intdef_updated)\n#Plot all\nplot(intdef_updated, \n     main = \"T-bill (3mo rate), inflation, and deficit (% of GDP)\",\n     legend.loc = \"topright\",)\n\n\n\n\nNow lets run the model again. Inflation plays a much more prominent role in the 3 month T-bill rate, than the deficit.\n\nupdated_model &lt;- lm(TB3MS ~ inflation + deficit, data = intdef_updated)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nTB3MS\n\n\n\n\n\n\n\n\ninflation\n\n\n0.83515*** (0.09277)\n\n\n\n\ndeficit\n\n\n0.28243*** (0.08660)\n\n\n\n\nConstant\n\n\n2.07613*** (0.52079)\n\n\n\n\n\n\n\n\nObservations\n\n\n63\n\n\n\n\nR2\n\n\n0.62114\n\n\n\n\nAdjusted R2\n\n\n0.60851\n\n\n\n\nResidual Std. Error\n\n\n2.02432 (df = 60)\n\n\n\n\nF Statistic\n\n\n49.18552*** (df = 2; 60)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-11-further-issues-in-using-ols-with-with-time-series-data",
    "href": "chapter_5.html#chapter-11-further-issues-in-using-ols-with-with-time-series-data",
    "title": "5  Regression models",
    "section": "5.11 Chapter 11: Further Issues in Using OLS with with Time Series Data",
    "text": "5.11 Chapter 11: Further Issues in Using OLS with with Time Series Data\n\n5.11.1 Example 11.7: Wages and Productivity\n\\[\\widehat{log(hrwage_t)} = \\beta_0 + \\beta_1log(outphr_t) + \\beta_2t + \\mu_t\\] Data from the Economic Report of the President, 1989, Table B-47. The data are for the non-farm business sector.\n\nwage_time &lt;- lm(lhrwage ~ loutphr + t, data = earns)\n\n\nwage_diff &lt;- lm(diff(lhrwage) ~ diff(loutphr), data = earns)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlhrwage\n\n\ndiff(lhrwage)\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nloutphr\n\n\n1.63964*** (0.09335)\n\n\n\n\n\n\nt\n\n\n-0.01823*** (0.00175)\n\n\n\n\n\n\ndiff(loutphr)\n\n\n\n\n0.80932*** (0.17345)\n\n\n\n\nConstant\n\n\n-5.32845*** (0.37445)\n\n\n-0.00366 (0.00422)\n\n\n\n\n\n\n\n\nObservations\n\n\n41\n\n\n40\n\n\n\n\nR2\n\n\n0.97122\n\n\n0.36424\n\n\n\n\nAdjusted R2\n\n\n0.96971\n\n\n0.34750\n\n\n\n\nResidual Std. Error (df = 38)\n\n\n0.02854\n\n\n0.01695\n\n\n\n\nF Statistic\n\n\n641.22430*** (df = 2; 38)\n\n\n21.77054*** (df = 1; 38)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-12-serial-correlation-and-heteroskedasticiy-in-time-series-regressions",
    "href": "chapter_5.html#chapter-12-serial-correlation-and-heteroskedasticiy-in-time-series-regressions",
    "title": "5  Regression models",
    "section": "5.12 Chapter 12: Serial Correlation and Heteroskedasticiy in Time Series Regressions",
    "text": "5.12 Chapter 12: Serial Correlation and Heteroskedasticiy in Time Series Regressions\n\n5.12.1 Example 12.8: Heteroskedasticity and the Efficient Markets Hypothesis\nThese are Wednesday closing prices of value-weighted NYSE average, available in many publications. Wooldridge does not recall the particular source used when he collected these data at MIT, but notes probably the easiest way to get similar data is to go to the NYSE web site, www.nyse.com.\n\\[return_t = \\beta_0 + \\beta_1return_{t-1} + \\mu_t\\]\n\nreturn_AR1 &lt;-lm(return ~ return_1, data = nyse)\n\n\\[\\hat{\\mu^2_t} = \\beta_0 + \\beta_1return_{t-1} + residual_t\\]\n\nreturn_mu &lt;- residuals(return_AR1)\nmu2_hat_model &lt;- lm(return_mu^2 ~ return_1, data = return_AR1$model)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nreturn\n\n\nreturn_mu2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nreturn_1\n\n\n0.05890 (0.03802)\n\n\n-1.10413*** (0.20140)\n\n\n\n\nConstant\n\n\n0.17963** (0.08074)\n\n\n4.65650*** (0.42768)\n\n\n\n\n\n\n\n\nObservations\n\n\n689\n\n\n689\n\n\n\n\nR2\n\n\n0.00348\n\n\n0.04191\n\n\n\n\nAdjusted R2\n\n\n0.00203\n\n\n0.04052\n\n\n\n\nResidual Std. Error (df = 687)\n\n\n2.11040\n\n\n11.17847\n\n\n\n\nF Statistic (df = 1; 687)\n\n\n2.39946\n\n\n30.05460***\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\n\n5.12.2 Example 12.9: ARCH in Stock Returns\n\\[\\hat{\\mu^2_t} = \\beta_0 + \\hat{\\mu^2_{t-1}} + residual_t\\]\nWe still have return_mu in the working environment so we can use it to create \\(\\hat{\\mu^2_t}\\), (mu2_hat) and \\(\\hat{\\mu^2_{t-1}}\\) (mu2_hat_1). Notice the use R’s matrix subset operations to perform the lag operation. We drop the first observation of mu2_hat and squared the results. Next, we remove the last observation of mu2_hat_1 using the subtraction operator combined with a call to the NROW function on return_mu. Now, both contain \\(688\\) observations and we can estimate a standard linear model.\n\nmu2_hat  &lt;- return_mu[-1]^2\nmu2_hat_1 &lt;- return_mu[-NROW(return_mu)]^2\narch_model &lt;- lm(mu2_hat ~ mu2_hat_1)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nmu2_hat\n\n\n\n\n\n\n\n\nmu2_hat_1\n\n\n0.33706*** (0.03595)\n\n\n\n\nConstant\n\n\n2.94743*** (0.44023)\n\n\n\n\n\n\n\n\nObservations\n\n\n688\n\n\n\n\nR2\n\n\n0.11361\n\n\n\n\nAdjusted R2\n\n\n0.11231\n\n\n\n\nResidual Std. Error\n\n\n10.75907 (df = 686)\n\n\n\n\nF Statistic\n\n\n87.92263*** (df = 1; 686)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-13-pooling-cross-sections-across-time-simple-panel-data-methods",
    "href": "chapter_5.html#chapter-13-pooling-cross-sections-across-time-simple-panel-data-methods",
    "title": "5  Regression models",
    "section": "5.13 Chapter 13: Pooling Cross Sections across Time: Simple Panel Data Methods",
    "text": "5.13 Chapter 13: Pooling Cross Sections across Time: Simple Panel Data Methods\n\n5.13.1 Example 13.7: Effect of Drunk Driving Laws on Traffic Fatalities\nWooldridge collected these data from two sources, the 1992 Statistical Abstract of the United States (Tables 1009, 1012) and A Digest of State Alcohol-Highway Safety Related Legislation, 1985 and 1990, published by the U.S. National Highway Traffic Safety Administration. \\[\\widehat{\\Delta{dthrte}} = \\beta_0 + \\Delta{open} + \\Delta{admin}\\]\n\nDD_model &lt;- lm(cdthrte ~ copen + cadmn, data = traffic1)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\ncdthrte\n\n\n\n\n\n\n\n\ncopen\n\n\n-0.41968** (0.20559)\n\n\n\n\ncadmn\n\n\n-0.15060 (0.11682)\n\n\n\n\nConstant\n\n\n-0.49679*** (0.05243)\n\n\n\n\n\n\n\n\nObservations\n\n\n51\n\n\n\n\nR2\n\n\n0.11867\n\n\n\n\nAdjusted R2\n\n\n0.08194\n\n\n\n\nResidual Std. Error\n\n\n0.34350 (df = 48)\n\n\n\n\nF Statistic\n\n\n3.23144** (df = 2; 48)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01"
  },
  {
    "objectID": "chapter_5.html#chapter-18-advanced-time-series-topics",
    "href": "chapter_5.html#chapter-18-advanced-time-series-topics",
    "title": "5  Regression models",
    "section": "5.14 Chapter 18: Advanced Time Series Topics",
    "text": "5.14 Chapter 18: Advanced Time Series Topics\n\n5.14.1 Example 18.8: FORECASTING THE U.S. UNEMPLOYMENT RATE\nData from Economic Report of the President, 2004, Tables B-42 and B-64.\n\\[\\widehat{unemp_t} = \\beta_0 + \\beta_1unem_{t-1}\\]\nEstimate the linear model in the usual way and note the use of the subset argument to define data equal to and before the year 1996.\n\nphillips_train &lt;- subset(phillips, year &lt;= 1996)\nunem_AR1 &lt;- lm(unem ~ unem_1, data = phillips_train)\n\n\\[\\widehat{unemp_t} = \\beta_0 + \\beta_1unem_{t-1} + \\beta_2inf_{t-1}\\]\n\nunem_inf_VAR1 &lt;- lm(unem ~ unem_1 + inf_1, data = phillips_train)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nunem\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nunem_1\n\n\n0.73235*** (0.09689)\n\n\n0.64703*** (0.08381)\n\n\n\n\ninf_1\n\n\n\n\n0.18358*** (0.04118)\n\n\n\n\nConstant\n\n\n1.57174*** (0.57712)\n\n\n1.30380** (0.48969)\n\n\n\n\n\n\n\n\nObservations\n\n\n48\n\n\n48\n\n\n\n\nR2\n\n\n0.55397\n\n\n0.69059\n\n\n\n\nAdjusted R2\n\n\n0.54427\n\n\n0.67684\n\n\n\n\nResidual Std. Error\n\n\n1.04857 (df = 46)\n\n\n0.88298 (df = 45)\n\n\n\n\nF Statistic\n\n\n57.13184*** (df = 1; 46)\n\n\n50.21941*** (df = 2; 45)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nNow, use the subset argument to create our testing data set containing observation after 1996. Next, pass the both the model object and the test set to the predict function for both models. Finally, cbind or “column bind” both forecasts as well as the year and unemployment rate of the test set.\n\nphillips_test &lt;- subset(phillips, year &gt;= 1997)\nAR1_forecast &lt;- predict.lm(unem_AR1, newdata = phillips_test)\nVAR1_forecast &lt;- predict.lm(unem_inf_VAR1, newdata = phillips_test)\nkable(cbind(phillips_test[ ,c(\"year\", \"unem\")], AR1_forecast, VAR1_forecast))\n\n\n\n\n\nyear\nunem\nAR1_forecast\nVAR1_forecast\n\n\n\n\n50\n1997\n4.9\n5.526452\n5.348468\n\n\n51\n1998\n4.5\n5.160275\n4.896451\n\n\n52\n1999\n4.2\n4.867333\n4.509137\n\n\n53\n2000\n4.0\n4.647627\n4.425175\n\n\n54\n2001\n4.8\n4.501157\n4.516062\n\n\n55\n2002\n5.8\n5.087040\n4.923537\n\n\n56\n2003\n6.0\n5.819394\n5.350271"
  },
  {
    "objectID": "chapter_6.html",
    "href": "chapter_6.html",
    "title": "6  Data tidying",
    "section": "",
    "text": "Let`s learn a consistent way to organize the data in R using a system called tidy data."
  },
  {
    "objectID": "chapter_6.html#prerequisites",
    "href": "chapter_6.html#prerequisites",
    "title": "6  Data tidying",
    "section": "6.1 Prerequisites",
    "text": "6.1 Prerequisites\nWe’ll focus on tidyr, a package that provides a bunch of tools to help tidy up your messy datasets. tidyr is a member of the core tidyverse. We will use pacman package to manage the necessary packages for this chapter.\n\npacman::p_load(tidyverse)"
  },
  {
    "objectID": "chapter_6.html#tidy-data",
    "href": "chapter_6.html#tidy-data",
    "title": "6  Data tidying",
    "section": "6.2 Tidy data",
    "text": "6.2 Tidy data\nYou can represent the same underlying data in multiple ways. The example below shows the same data organized in three different ways. Each dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\nThere are three interrelated rules that make a dataset tidy:\n\n\nEach variable is a column; each column is a variable.\n\n\nEach observation is a row; each row is an observation.\n\n\nEach value is a cell; each cell is a single value.\n\n\n\n\n\n\n\nThe following three rules make a dataset tidy: variables are columns, observations are rows, and values are cells\n\n\n\n\n\nWhy ensure that your data is tidy?\n\nThis is a consistent way of storing data. dplyr, ggplot2, and all the other packages in the tidyverse are designed to work with tidy data.\n\ntable1 &lt;- read.csv(\"data/table_1.csv\")\nhead(table1)\n\n      country year  cases population\n1 Afghanistan 1999    745   19987071\n2 Afghanistan 2000   2666   20595360\n3      Brazil 1999  37737  172006362\n4      Brazil 2000  80488  174504898\n5       China 1999 212258 1272915272\n6       China 2000 213766 1280428583\n\n\nHere are a few small examples showing how you might work with table1.\n\n# Compute rate per 10,000\ntable1 |&gt;\n  mutate(rate = cases / population * 10000)\n\n      country year  cases population     rate\n1 Afghanistan 1999    745   19987071 0.372741\n2 Afghanistan 2000   2666   20595360 1.294466\n3      Brazil 1999  37737  172006362 2.193930\n4      Brazil 2000  80488  174504898 4.612363\n5       China 1999 212258 1272915272 1.667495\n6       China 2000 213766 1280428583 1.669488\n\n\n\n# Compute total cases per year\ntable1 |&gt; \n  group_by(year) |&gt; \n  summarize(total_cases = sum(cases))\n\n# A tibble: 2 × 2\n   year total_cases\n  &lt;int&gt;       &lt;int&gt;\n1  1999      250740\n2  2000      296920\n\n\n\n# Visualize changes over time\nggplot(table1, aes(x = year, y = cases)) +\n  geom_line(aes(group = country), color = \"grey50\") +\n  geom_point(aes(color = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000)) # x-axis breaks at 1999 and 2000"
  },
  {
    "objectID": "chapter_3.html#prerequisites",
    "href": "chapter_3.html#prerequisites",
    "title": "3  Difference in Differences",
    "section": "3.1 Prerequisites",
    "text": "3.1 Prerequisites\nWe will use following packages for reproducing the contents of this chapter. For mamaging the necessary packages, we will use pacman package manager.\n\npacman::p_load(tidyverse,\n               knitr,\n               dplyr,\n               tidyr,\n               ggplot2,\n               readr,\n               kableExtra,\n               stargazer,\n               lmtest,\n               plm)"
  },
  {
    "objectID": "chapter_3.html#before-and-after-and-difference-in-differences-designs",
    "href": "chapter_3.html#before-and-after-and-difference-in-differences-designs",
    "title": "3  Difference in Differences",
    "section": "3.2 BEFORE-AND-AFTER AND DIFFERENCE-IN-DIFFERENCES DESIGNS",
    "text": "3.2 BEFORE-AND-AFTER AND DIFFERENCE-IN-DIFFERENCES DESIGNS\nIn observational studies, the data collected over time are a valuable source of information. Multiple measurements taken over time on the same units are called longitudinal data or panel data. Longitudinal data often yield a more credible comparison of the treatment and control groups than cross-section data because the former contain additional information about changes over time.\n\nThe before-and-after design examines how the outcome variable changed from the pretreatment period to the posttreatment period for the same set of units. The design is able to adjust for any confounding factor that is specific to each unit but does not change over time. However, the design does not address possible bias due to time-varying confounders.\nThe difference-in-differences (DiD) design extends the before-and-after design to address the confounding bias due to time trends. The key assumption behind the DiD design is that the outcome variable follows a parallel trend in the absence of treatment.\n\nFollowing figure graphically illustrates this assumption using the minimum-wage study data. The figure shows the outcome of interest, i.e., the average proportion of full-time employees, before and after the increase in the minimum wage for both the treatment group (fast-food restaurants in NJ, indicated by the solid black circles) and the control group (restaurants in PA, represented by the open black circles). In this setting, we can estimate the counterfactual outcome for the treatment group by assuming that the time trend for the treatment group is parallel to the observed trend for the control group. This estimate is indicated by the solid blue triangle.\n\n\n\n\n\nThe Difference-in-Differences Design in the Minimum-Wage Study\n\n\n\n\nIn the minimum- wage study, the researchers had collected the employment and wage information from the same set of restaurants before the minimum wage was increased in NJ. This pretreatment information allows several alternative designs for estimating causal effects in observational studies.\n\n3.2.1 Minimum wage and unemployment\nOur discussion of observational studies is based on the aforementioned minimum- wage debate. Two social science researchers examined the impact of raising the minimum wage on employment in the fast-food industry.3 In 1992, the state of New Jersey (NJ) in the United States raised the minimum wage from USD 4.25 to USD 5.05 per hour. Did such an increase in the minimum wage reduce employment as economic theory predicts?"
  },
  {
    "objectID": "chapter_5.html#chapter-2-the-simple-regression-model",
    "href": "chapter_5.html#chapter-2-the-simple-regression-model",
    "title": "5  Regression models",
    "section": "5.2 Chapter 2: The Simple Regression Model",
    "text": "5.2 Chapter 2: The Simple Regression Model\n\n5.2.1 Example 2.10: A Log Wage Equation\nLoad the wage1 data and check out the documentation. The documentation indicates these are data from the 1976 Current Population Survey, collected by Henry Farber when he and Wooldridge were colleagues at MIT in 1988.\n\\(educ\\): years of education\n\\(wage\\): average hourly earnings\n\\(lwage\\): log of the average hourly earnings\nFirst, make a scatter-plot of the two variables and look for possible patterns in the relationship between them.\n\n\n\n\n\nIt appears that on average, more years of education, leads to higher wages.\nThe example in the text is interested in the return to another year of education, or what the percentage change in wages one might expect for each additional year of education. To do so, one must use the \\(log(\\)wage\\()\\). This has already been computed in the data set and is defined as lwage.\nThe textbook provides excellent discussions around these topics, so please consult it.\nBuild a linear model to estimate the relationship between the log of wage (lwage) and education (educ).\n\\[\\widehat{log(wage)} = \\beta_0 + \\beta_1educ\\]\n\nlog_wage_model &lt;- lm(lwage ~ educ, data = wage1)\n\nPrint the summary of the results.\n\nsummary(log_wage_model)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlwage\n\n\n\n\n\n\n\n\neduc\n\n\n0.08274*** (0.00757)\n\n\n\n\nConstant\n\n\n0.58377*** (0.09734)\n\n\n\n\n\n\n\n\nObservations\n\n\n526\n\n\n\n\nR2\n\n\n0.18581\n\n\n\n\nAdjusted R2\n\n\n0.18425\n\n\n\n\nResidual Std. Error\n\n\n0.48008 (df = 524)\n\n\n\n\nF Statistic\n\n\n119.58160*** (df = 1; 524)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nPlot the \\(log(\\)wage\\()\\) vs educ. The blue line represents the least squares fit."
  },
  {
    "objectID": "chapter_5.html#chapter-5-multiple-regression-analysis-ols-asymptotics",
    "href": "chapter_5.html#chapter-5-multiple-regression-analysis-ols-asymptotics",
    "title": "5  Regression models",
    "section": "5.5 Chapter 5: Multiple Regression Analysis: OLS Asymptotics",
    "text": "5.5 Chapter 5: Multiple Regression Analysis: OLS Asymptotics\n\n5.5.1 Example 5.1: Housing Prices and Distance From an Incinerator\nLoad the hprice3 data set.\n\\(lprice:\\) Log(selling price)\n\\(ldist:\\) Log(distance from house to incinerator, feet)\n\\(larea:\\) Log(square footage of house)\nGraph the prices of housing against distance from an incinerator:\n\n\n\n\n\n\n\n\n\nNext, model the \\(log(\\)price\\()\\) against the \\(log(\\)dist\\()\\) to estimate the percentage relationship between the two.\n\\[price = \\alpha + \\beta_1 dist\\]\n\nprice_dist_model &lt;- lm(lprice ~ ldist, data = hprice3)\n\nCreate another model that controls for “quality” variables, such as square footage area per house.\n\\[price = \\alpha + \\beta_1 dist + \\beta_2 area\\]\n\nprice_area_model &lt;- lm(lprice ~ ldist + larea, data = hprice3)\n\nCompare the coefficients of both models. Notice that adding area improves the quality of the model, but also reduces the coefficient size of dist.\n\nsummary(price_dist_model)\nsummary(price_area_model)\n\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nlprice\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nldist\n\n\n0.31722*** (0.04811)\n\n\n0.19623*** (0.03816)\n\n\n\n\nlarea\n\n\n\n\n0.78368*** (0.05358)\n\n\n\n\nConstant\n\n\n8.25750*** (0.47383)\n\n\n3.49394*** (0.49065)\n\n\n\n\n\n\n\n\nObservations\n\n\n321\n\n\n321\n\n\n\n\nR2\n\n\n0.11994\n\n\n0.47385\n\n\n\n\nAdjusted R2\n\n\n0.11718\n\n\n0.47054\n\n\n\n\nResidual Std. Error\n\n\n0.41170 (df = 319)\n\n\n0.31883 (df = 318)\n\n\n\n\nF Statistic\n\n\n43.47673*** (df = 1; 319)\n\n\n143.19470*** (df = 2; 318)\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\nGraphing illustrates the larger coefficient for area."
  },
  {
    "objectID": "index.html#data-science-and-economics",
    "href": "index.html#data-science-and-economics",
    "title": "Data Science and Economics",
    "section": "Data Science and Economics",
    "text": "Data Science and Economics\n\n\n\n\n\n\n\n\n\nWelcome to the captivating world where data science intersects with economics! In an era defined by vast amounts of data and an increasing reliance on data-driven decision-making, the synergy between these two fields has become paramount. The fusion of data science and economics has brought about a paradigm shift, enabling us to unravel hidden patterns, make precise predictions, and unlock unprecedented insights into the workings of our economic systems.\nThis course, “Data Science and Economics,” is designed to be a comprehensive exploration of the powerful union between these two disciplines. It aims to equip you with the fundamental knowledge and practical skills necessary to leverage data science techniques for economic analysis, policy formulation, and business strategy. Whether you are an aspiring economist, a data enthusiast seeking to enter the world of economics, or a seasoned professional looking to expand your analytical toolkit, this course will serve as your guide to navigate the intricate terrain of data-driven economic analysis.\nAs you embark on this educational journey, you will discover the intrinsic relationship between data science and economics. You will delve into the foundations of economics, gaining a deep understanding of macroeconomics, microeconomics, and the principles that shape our economic landscape. Simultaneously, you will acquire a solid grasp of essential data science concepts, such as data collection, cleaning, visualization, statistical analysis, and machine learning. These interconnected disciplines will empower you to harness the full potential of data to tackle complex economic problems, explore market dynamics, forecast trends, and inform evidence-based policy decisions.\nThroughout this course, you will be exposed to real-world case studies, engaging exercises, and hands-on projects that bridge the gap between theory and practice. You will learn how to effectively leverage economic data sets, analyze their nuances, and extract meaningful insights to answer critical questions. Moreover, you will acquire the skills to communicate your findings persuasively, presenting economic analyses in a concise and compelling manner to stakeholders and decision-makers.\nIt is important to note that this course is not only focused on mastering technical tools and methodologies. It also emphasizes the ethical considerations and responsible use of data science in the realm of economics. As we navigate the ever-expanding landscape of data, we must be cognizant of the potential biases, limitations, and social implications that arise from our analyses. By fostering a holistic understanding of the ethical dimensions, we aim to cultivate responsible data scientists who can leverage their skills for the betterment of society.\nIn conclusion, “Data Science and Economics” is an invitation to explore the captivating intersection of data science and economics, where the power of information merges with the complexities of economic systems. By immersing yourself in this course, you will unlock a world of analytical possibilities and develop a versatile skill set that will shape the future of economic analysis. We hope that this journey will inspire you to embrace the boundless opportunities presented by the fusion of data science and economics, empowering you to make informed decisions, unravel economic mysteries, and drive positive change in the world around us."
  },
  {
    "objectID": "chapter_7.html",
    "href": "chapter_7.html",
    "title": "7  Visualization of qualitative data",
    "section": "",
    "text": "Visualization can either be made with vectors and tables but also though diagrams like histograms, bar charts, pie charts etc. In each case, it is quite important to distinct our observations in quantitative or qualitative data. We start with qualitative data, which are usually represented with tables, bar charts and pie charts.\nExample: Let’s assume we have the answers given by 20 persons to the question: “What means of transportation do you use every day to go to your job?”. Respondents had to choose between car, bus, metro and foot. The answers given where:\n\ncar, car, bus, metro, metro, car, metro, metro, foot, car, foot, bus, bus, metro, metro, car, car, car, metro, car\n\nWe initially insert data by using the m vector. Our data are categorical so we should add these values inside double quotes:\n\nm = c(\"car\", \"car\", \"bus\", \"metro\", \"metro\", \"car\", \"metro\", \"metro\", \"foot\", \"car\", \"foot\", \"bus\", \"bus\", \"metro\", \"metro\", \"car\", \"car\", \"car\", \"metro\", \"car\")\n\nFREQUENCY TABLE: We can represent our data in a frequency table by using the table() command.\n\ntable(m)\n\nm\n  bus   car  foot metro \n    3     8     2     7 \n\n\nIf we wanted to view the relative frequency then we could use the prop.table(table()) command.\n\nprop.table(table(m))\n\nm\n  bus   car  foot metro \n 0.15  0.40  0.10  0.35 \n\n\nBAR CHARTS: Frequency bar charts can be displayed with the command:\n\nbarplot(table(m))\n\n\n\n\nRelative frequency bar charts can be displayed with the command:\n\nbarplot(prop.table(table(m)))\n\n\n\n\nPIE CHART: We can display our data in a pie chart by using the command:\n\npie(table(m))\n\n\n\n\n\n7.0.1 Contigency Matrix\nA contingency matrix has to do with two categorical variables and displays their frequency distribution.\n\nLet’s assume that in the previous example’s data we have added the gender of each person.\nFor convenience, let’s assume that the first 8 people were male (M) and the remaining 12 were female (F).\n\nWe create the vector g:\n\ng = c(rep(\"M\", 8), rep(\"F\", 12))\ng\n\n [1] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"M\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\" \"F\"\n[20] \"F\"\n\n\nWe create the double input frequency table mg like this:\n\nmg = table(m,g)\nmg\n\n       g\nm       F M\n  bus   2 1\n  car   5 3\n  foot  2 0\n  metro 3 4\n\n\nThen, using the table, we calculate marginal frequencies in terms of means of transportation and gender, respectively:\n\nmargin.table(mg,1)\n\nm\n  bus   car  foot metro \n    3     8     2     7 \n\n\n\nmargin.table(mg,2)\n\ng\n F  M \n12  8 \n\n\nWe can work with the relative frequency table with the exact same way.\n\nprop.table(mg)\n\n       g\nm          F    M\n  bus   0.10 0.05\n  car   0.25 0.15\n  foot  0.10 0.00\n  metro 0.15 0.20\n\n\n\nprop.table(mg,1)\n\n       g\nm               F         M\n  bus   0.6666667 0.3333333\n  car   0.6250000 0.3750000\n  foot  1.0000000 0.0000000\n  metro 0.4285714 0.5714286\n\n\n\nprop.table(mg,2)\n\n       g\nm               F         M\n  bus   0.1666667 0.1250000\n  car   0.4166667 0.3750000\n  foot  0.1666667 0.0000000\n  metro 0.2500000 0.5000000\n\n\n\n\n7.0.2 Stacked bar charts and grouped bar charts\nWe can display qualitative data coming from the values of two variables by using a stacked bar chart or a grouped bar chart.\n\nbarplot(mg)\n\n\n\n\nBy using the command parameters, the result is much better:\n\nbarplot(mg, xlim=c(0,2), xlab=\"Gender\",\n        legend =levels(m), col =1:4)\n\n\n\n\n\nbarplot(mg, xlim = c(0,2), xlab = \"Transportation\",\n        legend = levels(g), col=1:2)\n\n\n\n\nThe below commands create grouped bar charts.\n\nbarplot(prop.table(mg,1), width = 0.25,\n        xlim = c(0,3), ylim = c(0,1), xlab = \"Gender\",\n        legend = levels(m), beside = T, col = 1:4)\n\n\n\n\n\nmg = table(g,m)\nbarplot(prop.table(mg,2), width = 0.25,\n       xlim = c(0,3), ylim = c(0,1), xlab = \"Transportation\",\n       legend = levels(g), beside = T, col = 1:2)"
  }
]